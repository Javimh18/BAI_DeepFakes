{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284e4dbc",
   "metadata": {},
   "source": [
    "# CLASE (IMAGENES REALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997245ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, resize=128, zoom_factor=0):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.zoom_factor = zoom_factor\n",
    "        self.mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.images = []\n",
    "        \n",
    "        # Cargar los datos\n",
    "        for img_name in os.listdir(root_dir):\n",
    "            img_path = os.path.join(root_dir, img_name)\n",
    "            self.load_image_faces(img_path)\n",
    "\n",
    "    def load_image_faces(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        boxes, _ = self.mtcnn.detect(img)\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                # Aquí modificamos el padding para hacer zoom out\n",
    "                padding_x = (box[2] - box[0]) * self.zoom_factor\n",
    "                padding_y = (box[3] - box[1]) * self.zoom_factor\n",
    "                new_box = [\n",
    "                    max(box[0] - padding_x, 0),  # xmin\n",
    "                    max(box[1] - padding_y, 0),  # ymin\n",
    "                    min(box[2] + padding_x, img.width),  # xmax\n",
    "                    min(box[3] + padding_y, img.height)  # ymax\n",
    "                ]\n",
    "                face = img.crop(new_box).resize((self.resize, self.resize))\n",
    "                self.images.append(face)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        face = self.images[idx]\n",
    "        if self.transform:\n",
    "            face = self.transform(face)\n",
    "        return face\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set = CustomFaceDataset(root_dir='/home/alumnos/e400777/100k', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Contador para las primeras 10 imágenes\n",
    "images_count = 0\n",
    "\n",
    "for images in train_loader:\n",
    "    for image in images:\n",
    "        images_count += 1\n",
    "        image_np = image.numpy().transpose((1, 2, 0))\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(image_np)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        if images_count == 10:\n",
    "            break\n",
    "    if images_count == 10:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953bc68f-22a9-45d4-b9da-7c55dfd7ed1e",
   "metadata": {},
   "source": [
    "# CLASE (IMAGENES REALES) TEST (SOLO PARA VISUALIZACION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd5833-c2f3-439d-a730-214dd49473a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, resize=128, zoom_factor=0):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.zoom_factor = zoom_factor\n",
    "        self.mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.images = []\n",
    "        \n",
    "        # Cargar los datos\n",
    "        for img_name in os.listdir(root_dir):\n",
    "            img_path = os.path.join(root_dir, img_name)\n",
    "            self.load_image_faces(img_path)\n",
    "\n",
    "    def load_image_faces(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        boxes, _ = self.mtcnn.detect(img)\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                # Aquí modificamos el padding para hacer zoom out\n",
    "                padding_x = (box[2] - box[0]) * self.zoom_factor\n",
    "                padding_y = (box[3] - box[1]) * self.zoom_factor\n",
    "                new_box = [\n",
    "                    max(box[0] - padding_x, 0),  # xmin\n",
    "                    max(box[1] - padding_y, 0),  # ymin\n",
    "                    min(box[2] + padding_x, img.width),  # xmax\n",
    "                    min(box[3] + padding_y, img.height)  # ymax\n",
    "                ]\n",
    "                face = img.crop(new_box).resize((self.resize, self.resize))\n",
    "                self.images.append(face)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        face = self.images[idx]\n",
    "        if self.transform:\n",
    "            face = self.transform(face)\n",
    "        return face\n",
    "\n",
    "\n",
    "# Crea tu dataset\n",
    "test_set_v = CustomFaceDataset(root_dir='/home/alumnos/e400777/Image-Classification-Using-Vision-transformer-main/Task_2_3/evaluation/real', transform=transform)\n",
    "\n",
    "# Asegúrate de que realmente esté encontrando imágenes\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError(\"El dataset está vacío. Verifica la ruta del directorio y el formato de las imágenes.\")\n",
    "\n",
    "# Crea un DataLoader\n",
    "test_loader_v = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Ahora puedes iterar sobre data_loader para obtener tus lotes de imágenes y etiquetas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Contador para las primeras 10 imágenes\n",
    "images_count = 0\n",
    "\n",
    "# Iteramos sobre el data_loader\n",
    "for images in data_loader:\n",
    "    # Iteramos sobre cada imagen en el lote (batch)\n",
    "    for image in images:\n",
    "        # Incrementamos el contador\n",
    "        images_count += 1\n",
    "        # Convertimos la imagen de Tensor a NumPy y cambiamos el orden de los ejes para visualización\n",
    "        image_np = image.numpy().transpose((1, 2, 0))\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(image_np)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Si ya mostramos 10 imágenes, salimos del bucle\n",
    "        if images_count == 20:\n",
    "            break\n",
    "    # Si ya mostramos 10 imágenes, salimos del bucle externo\n",
    "    if images_count == 20:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9abe9e-c6bf-461e-a145-090cb6575e20",
   "metadata": {},
   "source": [
    "# CLASE TEST (REALES Y FALSAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0a0ef-6a3c-4ada-aaf4-5894d1ff7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class DeepfakeFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, resize=128,zoom_factor=0):\n",
    "        \"\"\"\n",
    "        root_dir (string): Directorio con todas las imágenes, organizadas en subdirectorios 'real' y 'fake'.\n",
    "        transform (callable, optional): Opcional transformación para ser aplicada en una cara.\n",
    "        resize (int): Tamaño al cual se redimensionará la cara detectada.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.zoom_factor = zoom_factor\n",
    "        self.mtcnn = MTCNN(keep_all=True)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Cargar los datos\n",
    "        for label, cls in enumerate(['real', 'fake']):\n",
    "            cls_folder = os.path.join(root_dir, cls)\n",
    "            for img_name in os.listdir(cls_folder):\n",
    "                img_path = os.path.join(cls_folder, img_name)\n",
    "                self.load_image_faces(img_path, label)\n",
    "\n",
    "    def load_image_faces(self, img_path, label):\n",
    "        img = Image.open(img_path)\n",
    "        boxes, _ = self.mtcnn.detect(img)\n",
    "        if boxes is not None:\n",
    "            for box in boxes:  \n",
    "                padding_x = (box[2] - box[0]) * self.zoom_factor\n",
    "                padding_y = (box[3] - box[1]) * self.zoom_factor\n",
    "                new_box = [\n",
    "                    max(box[0] - padding_x, 0),  # xmin\n",
    "                    max(box[1] - padding_y, 0),  # ymin\n",
    "                    min(box[2] + padding_x, img.width),  # xmax\n",
    "                    min(box[3] + padding_y, img.height)  # ymax\n",
    "                ]\n",
    "                face = img.crop(new_box).resize((self.resize, self.resize))\n",
    "                self.data.append(face)\n",
    "                self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        face = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            face = self.transform(face)\n",
    "\n",
    "        return face, label\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "test_set = DeepfakeFaceDataset(root_dir='/home/alumnos/e400777/Image-Classification-Using-Vision-transformer-main/Task_2_3/evaluation', transform=transform)\n",
    "test_loader = DataLoader(val_set, batch_size=1, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for patches, labels in test_loader:\n",
    "    if i > 30:\n",
    "        break\n",
    "    patch = patches.squeeze(0)  \n",
    "    plt.imshow(patch.numpy().transpose(1, 2, 0))  \n",
    "    plt.title(f'Label: {\"Real\" if labels.item() == 0 else \"Fake\"}') \n",
    "    plt.show()\n",
    "    i += 1\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb3763-2bb3-4828-aedd-743ed938b3bc",
   "metadata": {},
   "source": [
    "# AUTOENCODER NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, codings_size=100, EPSILON=0.00005, dropout_p=0.1):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Codificador\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*8*1024, codings_size),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Decodificador\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(codings_size, 8*8*1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (1024, 8, 8)),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(512, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(256, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(3, eps=EPSILON),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "autoencoder = ConvAutoencoder()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "autoencoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.005)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_per_epoch = 0\n",
    "    for data in train_loader:\n",
    "        images = data  \n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = autoencoder(images)\n",
    "        \n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    \n",
    "    epoch_loss = loss_per_epoch / len(data_loader)\n",
    "    print(f\"Época [{epoch+1}/{epochs}], Pérdida: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5850b2-5691-4b70-b869-5e6d224bddb0",
   "metadata": {},
   "source": [
    "## VISUALIZAR IN/OUT IMAGENES TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images = next(dataiter)  \n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    reconstructed = autoencoder(images)\n",
    "\n",
    "images = images.cpu()\n",
    "reconstructed = reconstructed.cpu()\n",
    "\n",
    "n_images = 10  # Número de imágenes a visualizar\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_images, figsize=(20, 4))\n",
    "for i in range(n_images):\n",
    "    # Imagen original\n",
    "    ax = axes[0, i]\n",
    "    ax.imshow(images[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Original\")\n",
    "\n",
    "    # Imagen reconstruida\n",
    "    ax = axes[1, i]\n",
    "    ax.imshow(reconstructed[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275818b-57fb-400c-b8b3-eacbce1c9084",
   "metadata": {},
   "source": [
    "## VISUALIZAR IN/OUT IMAGENES TEST (DA IGUAL REALES O FALSAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images = next(dataiter)  \n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    reconstructed = autoencoder(images)\n",
    "\n",
    "images = images.cpu()\n",
    "reconstructed = reconstructed.cpu()\n",
    "\n",
    "n_images = 10  # Número de imágenes a visualizar\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_images, figsize=(20, 4))\n",
    "for i in range(n_images):\n",
    "    # Imagen original\n",
    "    ax = axes[0, i]\n",
    "    ax.imshow(images[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Original\")\n",
    "\n",
    "    # Imagen reconstruida\n",
    "    ax = axes[1, i]\n",
    "    ax.imshow(reconstructed[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7055e-eb90-4be8-8c7a-b23c7ff235cf",
   "metadata": {},
   "source": [
    "# TEST EN AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval() \n",
    "\n",
    "errores_reales = []\n",
    "errores_fakes = []\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for imagenes, etiquetas in test_loader:\n",
    "        imagenes = imagenes.to(device)\n",
    "        etiquetas = etiquetas.to(device)\n",
    "\n",
    "        reconstrucciones = autoencoder(imagenes)\n",
    "\n",
    "        # Calcula (MSE) \n",
    "        errores = ((imagenes - reconstrucciones)**2).mean(dim=[1, 2, 3]).cpu().numpy()\n",
    "\n",
    "        # Separa errores x clase\n",
    "        for error, etiqueta in zip(errores, etiquetas):\n",
    "            if etiqueta == 0:  #'0' es la etiqueta para imágenes reales\n",
    "                errores_reales.append(error)\n",
    "            else:  #cualquier otro valor es para imágenes falsas\n",
    "                errores_fakes.append(error)\n",
    "\n",
    "# Calcula la media de los errores para cada clase\n",
    "media_errores_reales = sum(errores_reales) / len(errores_reales)\n",
    "media_errores_fakes = sum(errores_fakes) / len(errores_fakes)\n",
    "\n",
    "print(f\"Media de errores para imágenes reales: {media_errores_reales}\")\n",
    "print(f\"Media de errores para imágenes falsas: {media_errores_fakes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7966a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "errores_reales = np.array(errores_reales)\n",
    "errores_fakes = np.array(errores_fakes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(errores_reales, bins=50, alpha=0.5, label='Reales', color='green')\n",
    "\n",
    "# Histograma para los errores de imágenes falsas\n",
    "plt.hist(errores_fakes, bins=50, alpha=0.5, label='Falsas', color='red')\n",
    "\n",
    "plt.xlabel('Error de Reconstrucción')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de los Errores de Reconstrucción')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8efef0-1577-411c-9c7b-59fae5e1a0c4",
   "metadata": {},
   "source": [
    "## INTENTO DE EVALUACION AJUSTANDO UMBRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral = 0.002\n",
    "\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "predicciones = []\n",
    "etiquetas_reales = []\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for data in test_loader:\n",
    "        imagenes, etiquetas = data\n",
    "        imagenes = imagenes.to(device)\n",
    "        \n",
    "        reconstrucciones = autoencoder(imagenes)\n",
    "        \n",
    "        error = ((imagenes - reconstrucciones)**2).mean([1, 2, 3])  # Calcula el error por imagen\n",
    "        \n",
    "        predicciones_batch = error > umbral\n",
    "        predicciones.extend(predicciones_batch.cpu().numpy())\n",
    "        etiquetas_reales.extend(etiquetas.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(etiquetas_reales, predicciones, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b6204",
   "metadata": {},
   "source": [
    "# DENOISING AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9fda30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, codings_size=100, EPSILON=0.00005, dropout_p=0.4):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Codificador\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*8*1024, codings_size),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Decodificador\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(codings_size, 8*8*1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (1024, 8, 8)),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(512, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(256, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64, eps=EPSILON),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(3, eps=EPSILON),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def add_noise(imgs, noise_factor=0.5):\n",
    "    \"\"\"Añade ruido gaussiano a las imágenes.\"\"\"\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)  \n",
    "    return noisy_imgs\n",
    "def pixelate_images(imgs, pixelation_level=0.5):\n",
    "    \"\"\"Pixela un batch de imágenes.\"\"\"\n",
    "    batch_size, c, h, w = imgs.size()\n",
    "    new_h, new_w = int(h * pixelation_level), int(w * pixelation_level)\n",
    "    imgs = F.interpolate(imgs, size=(new_h, new_w), mode='nearest')\n",
    "    imgs = F.interpolate(imgs, size=(h, w), mode='nearest')\n",
    "    return imgs\n",
    "\n",
    "def add_combined_noise(imgs, gaussian_noise_factor=0.5, pixelation_level=0.3):\n",
    "    \"\"\"Añade una combinación de ruido gaussiano y pixelado a las imágenes.\"\"\"\n",
    "    noisy_imgs = add_gaussian_noise(imgs, noise_factor=gaussian_noise_factor)\n",
    "    noisy_imgs = pixelate_images(noisy_imgs, pixelation_level=pixelation_level)  \n",
    "    return noisy_imgs\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "autoencoder = ConvAutoencoder()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "autoencoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.05)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_per_epoch = 0\n",
    "    for data in train_loader:\n",
    "        images = data  \n",
    "        images = images.to(device)\n",
    "        \n",
    "        noisy_images = add_combined_noise(images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = autoencoder(noisy_images)\n",
    "        \n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    \n",
    "    epoch_loss = loss_per_epoch / len(data_loader)\n",
    "    print(f\"Época [{epoch+1}/{epochs}], Pérdida: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbd10d",
   "metadata": {},
   "source": [
    "## VISUALIZAR IN/OUT IMAGENES TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images = next(dataiter)  \n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    reconstructed = autoencoder(images)\n",
    "\n",
    "images = images.cpu()\n",
    "reconstructed = reconstructed.cpu()\n",
    "\n",
    "n_images = 10  # Número de imágenes a visualizar\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_images, figsize=(20, 4))\n",
    "for i in range(n_images):\n",
    "    # Imagen original\n",
    "    ax = axes[0, i]\n",
    "    ax.imshow(images[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Original\")\n",
    "\n",
    "    # Imagen reconstruida\n",
    "    ax = axes[1, i]\n",
    "    ax.imshow(reconstructed[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a88030",
   "metadata": {},
   "source": [
    "## VISUALIZAR IN/OUT IMAGENES TEST (DA IGUAL REALES O FALSAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images = next(dataiter)  \n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    reconstructed = autoencoder(images)\n",
    "\n",
    "images = images.cpu()\n",
    "reconstructed = reconstructed.cpu()\n",
    "\n",
    "n_images = 10  # Número de imágenes a visualizar\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_images, figsize=(20, 4))\n",
    "for i in range(n_images):\n",
    "    # Imagen original\n",
    "    ax = axes[0, i]\n",
    "    ax.imshow(images[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Original\")\n",
    "\n",
    "    # Imagen reconstruida\n",
    "    ax = axes[1, i]\n",
    "    ax.imshow(reconstructed[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fb891",
   "metadata": {},
   "source": [
    "# TEST EN DENOISING AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval() \n",
    "\n",
    "errores_reales = []\n",
    "errores_fakes = []\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for imagenes, etiquetas in test_loader:\n",
    "        imagenes = imagenes.to(device)\n",
    "        etiquetas = etiquetas.to(device)\n",
    "\n",
    "        reconstrucciones = autoencoder(imagenes)\n",
    "\n",
    "        # Calcula (MSE) \n",
    "        errores = ((imagenes - reconstrucciones)**2).mean(dim=[1, 2, 3]).cpu().numpy()\n",
    "\n",
    "        # Separa errores x clase\n",
    "        for error, etiqueta in zip(errores, etiquetas):\n",
    "            if etiqueta == 0:  #'0' es la etiqueta para imágenes reales\n",
    "                errores_reales.append(error)\n",
    "            else:  #cualquier otro valor es para imágenes falsas\n",
    "                errores_fakes.append(error)\n",
    "\n",
    "# Calcula la media de los errores para cada clase\n",
    "media_errores_reales = sum(errores_reales) / len(errores_reales)\n",
    "media_errores_fakes = sum(errores_fakes) / len(errores_fakes)\n",
    "\n",
    "print(f\"Media de errores para imágenes reales: {media_errores_reales}\")\n",
    "print(f\"Media de errores para imágenes falsas: {media_errores_fakes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a968ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral = 0.002\n",
    "\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "predicciones = []\n",
    "etiquetas_reales = []\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for data in test_loader:\n",
    "        imagenes, etiquetas = data\n",
    "        imagenes = imagenes.to(device)\n",
    "        \n",
    "        reconstrucciones = autoencoder(imagenes)\n",
    "        \n",
    "        error = ((imagenes - reconstrucciones)**2).mean([1, 2, 3])  # Calcula el error por imagen\n",
    "        \n",
    "        predicciones_batch = error > umbral\n",
    "        predicciones.extend(predicciones_batch.cpu().numpy())\n",
    "        etiquetas_reales.extend(etiquetas.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(etiquetas_reales, predicciones, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4be90a",
   "metadata": {},
   "source": [
    "## PROBAR RUIDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def add_noise(imgs, noise_factor=0.5): \n",
    "    \"\"\"Añade ruido gaussiano a las imágenes.\"\"\"\n",
    "    noisy_imgs = imgs + noise_factor * torch.randn_like(imgs)\n",
    "    noisy_imgs = torch.clamp(noisy_imgs, 0., 1.) \n",
    "    return noisy_imgs\n",
    "\n",
    "def pixelate_image(img, pixelation_level=0.5):\n",
    "    _, h, w = img.size()\n",
    "    new_h, new_w = int(h * pixelation_level), int(w * pixelation_level)\n",
    "    img = F.interpolate(img.unsqueeze(0), size=(new_h, new_w), mode='nearest').squeeze(0)\n",
    "    img = F.interpolate(img.unsqueeze(0), size=(h, w), mode='nearest').squeeze(0)\n",
    "    return img\n",
    "def add_combined_noise(imgs, gaussian_noise_factor=0.3, pixelation_level=0.5):\n",
    "    \"\"\"Añade una combinación de ruido gaussiano y pixelado a las imágenes.\"\"\"\n",
    "    noisy_imgs = add_noise(imgs, noise_factor=gaussian_noise_factor)\n",
    "    noisy_imgs = pixelate_image(noisy_imgs, pixelation_level=pixelation_level)\n",
    "    return noisy_imgs\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for patches, labels in tes_loader:\n",
    "    if i > 9:  \n",
    "        break\n",
    "\n",
    "    patch = patches.squeeze(0)  \n",
    "\n",
    "    # Añade ruido a la imagen\n",
    "    noisy_patch = add_combined_noise(patch)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(patch.numpy().transpose(1, 2, 0))\n",
    "    plt.title(f'Original - Label: {\"Real\" if labels.item() == 0 else \"Fake\"}')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(noisy_patch.numpy().transpose(1, 2, 0))\n",
    "    plt.title(f'Noisy - Label: {\"Real\" if labels.item() == 0 else \"Fake\"}')\n",
    "    plt.show()\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7cdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
